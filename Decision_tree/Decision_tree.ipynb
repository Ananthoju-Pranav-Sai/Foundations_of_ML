{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# FoML Assign 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Ananthoju-Pranav-Sai\" # or \"Amar-Akbar-Antony\"\n",
    "used_feature = [0,0,0,0,0,0,0,0,0,0,0]  #we use this to keep track of whether the feature is used or not for comparision\n",
    "# Implement your decision tree below\n",
    "class DecisionTree():\n",
    "    tree = {}\n",
    "    global used_feature  \n",
    "    def learn(self, training_set):\n",
    "        # implement this function\n",
    "        tree={}\n",
    "        tree['feat_index']=-1               \n",
    "        tree['threshold']=-1\n",
    "        tree['subtree']={}\n",
    "        subtree={}\n",
    "        X,Y = [row[:-1] for row in training_set],[row[11]for row in training_set]\n",
    "        n_sam,n_feat = np.shape(X)\n",
    "\n",
    "        if(n_sam>=2 and self.entropy(Y)!=0):    #stopping conditions\n",
    "\n",
    "            best_split = self.Best_split(training_set,n_sam,n_feat)\n",
    "            if(best_split[\"feat_index\"]!=-1):                   \n",
    "                tree['feat_index']=best_split[\"feat_index\"] \n",
    "                tree['threshold']=best_split[\"threshold\"]    \n",
    "                subtree['left']=DecisionTree.learn(self,best_split[\"left_dataset\"])                     \n",
    "                subtree['right']=DecisionTree.learn(self,best_split[\"right_dataset\"])  \n",
    "                subtree['value']=None\n",
    "                tree['subtree']=subtree   \n",
    "                self.tree = tree\n",
    "                # print(self.tree)\n",
    "                return tree\n",
    "\n",
    "        leaf_value = max(Y,key=Y.count) \n",
    "        subtree['left']=None\n",
    "        subtree['right']=None\n",
    "        subtree['value']=leaf_value                                  #if the impurity is zero then we are in the leaf node. \n",
    "        tree['subtree']=subtree \n",
    "        self.tree=tree\n",
    "        return tree\n",
    "\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self,tree,test_instance):\n",
    "\n",
    "        if(tree['subtree']['value']!=None):   #if the node is leaf it just returns its value\n",
    "            return tree['subtree']['value']\n",
    "        feat_value = test_instance[tree['feat_index']]           \n",
    "        if feat_value<=tree['threshold']:       #if not then it will check its value with threshold and goes left/right subtree accordingly \n",
    "            return self.classify(tree['subtree']['left'],test_instance)\n",
    "        else:\n",
    "            return self.classify(tree['subtree']['right'],test_instance)\n",
    "\n",
    "    def split_dataset(self,dataset,feat_index,threshold):           #splits the dataset according to the feature and its threshold\n",
    "        left = np.array([row for row in dataset if (row[feat_index])<=threshold])\n",
    "        right = np.array([row for row in dataset if (row[feat_index])>threshold])\n",
    "        return left, right\n",
    "\n",
    "    def Best_split(self,dataset,n_sam,n_feat):  #finds the best split i.e., split with max info gain using all features \n",
    "        best_split = {}\n",
    "        max_info_gain = -np.inf\n",
    "        #defining the attributes of best_split\n",
    "        best_split[\"feat_index\"] = -1\n",
    "        best_split[\"threshold\"] = -1\n",
    "        best_split[\"left_dataset\"] = []\n",
    "        best_split[\"right_dataset\"] = []\n",
    "\n",
    "        for feat_index in range(n_feat):\n",
    "            feat_values = [row[feat_index] for row in dataset]\n",
    "            d = float(max(feat_values)-min(feat_values))/25\n",
    "            possible_thresholds = np.linspace(float(min(feat_values))+d,float(max(feat_values))-d,10) \n",
    "            for t in possible_thresholds:\n",
    "                left_dataset,right_dataset = self.split_dataset(dataset,feat_index,t)\n",
    "                if(len(left_dataset)>0 and len(right_dataset)>0):\n",
    "                    y,y_left,y_right = [row[11]for row in dataset],[row[11]for row in left_dataset],[row[11]for row in left_dataset]\n",
    "                    info_gain_t = self.info_gain(y,y_left,y_right,\"entropy\")\n",
    "                    if(info_gain_t>max_info_gain):\n",
    "                        best_split[\"feat_index\"] = feat_index\n",
    "                        best_split[\"threshold\"] = t\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        max_info_gain=info_gain_t\n",
    "        return best_split\n",
    "\n",
    "    def info_gain(self,parent,child_1,child_2,way):  #function used to calculate info_gain\n",
    "        w1 = len(child_1)/len(parent)\n",
    "        w2 = len(child_2)/len(parent)\n",
    "        \n",
    "        if(way==\"entropy\"):\n",
    "            gain = self.entropy(parent)-(w1*self.entropy(child_1)+w2*self.entropy(child_2))\n",
    "        if(way==\"gini_index\"):\n",
    "            gain = self.gini_index(parent)-(w1*self.gini_index(child_1)+w2*self.gini_index(child_2))\n",
    "        return gain\n",
    "\n",
    "    def entropy(self,label):        #function to calculate impurity using entropy\n",
    "        label = np.array(label,dtype=int)\n",
    "        classes = np.unique(label)\n",
    "        entropy = 0\n",
    "        for i in classes:\n",
    "            p_c = len(label[label == i]) / len(label)\n",
    "            entropy += -p_c * np.log2(p_c)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self,label):     #function to calculate impurity using gini index\n",
    "        label = np.array(label,dtype=int)\n",
    "        classes = np.unique(label)\n",
    "        s=0\n",
    "        for i in classes:\n",
    "            p_c = len(label[label == i]) / len(label)\n",
    "            s += p_c*p_c\n",
    "        gini = 1-s\n",
    "        return gini\n",
    "    \n",
    "    def print_tree(self):       #function to print the tree\n",
    "        print(self.tree['feat_index'],self.tree['threshold'])\n",
    "        if(self.tree['value']!=-1):\n",
    "            print(self.tree['value'])\n",
    "        else:\n",
    "            self.tree['left'].print_tree()\n",
    "            self.tree['right'].print_tree()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "        data = np.array(data,dtype=float)\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    accuracy=[]\n",
    "    for j in range(10):\n",
    "        K = 10\n",
    "        training_set = [x for i, x in enumerate(data) if i % K != j]\n",
    "        test_set = [x for i, x in enumerate(data) if i % K == j]\n",
    "        \n",
    "        Tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        tree = Tree.learn( training_set )\n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = Tree.classify(tree,instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "                # print(result==instance[-1],result)\n",
    "\n",
    "            # Accuracy\n",
    "        accuracy.append(float(results.count(True))/float(len(results)))\n",
    "        print(\"Accuracy in \",j+1,\" fold : \",accuracy[j])\n",
    "    accuracy = float(sum(accuracy)/len(accuracy))\n",
    "    print (\"accuracy: %.4f\" % accuracy)    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of records: 4898\n",
      "Accuracy in  1  fold :  0.8285714285714286\n",
      "Accuracy in  2  fold :  0.8224489795918367\n",
      "Accuracy in  3  fold :  0.8081632653061225\n",
      "Accuracy in  4  fold :  0.8367346938775511\n",
      "Accuracy in  5  fold :  0.8489795918367347\n",
      "Accuracy in  6  fold :  0.8367346938775511\n",
      "Accuracy in  7  fold :  0.8387755102040816\n",
      "Accuracy in  8  fold :  0.8204081632653061\n",
      "Accuracy in  9  fold :  0.8323108384458078\n",
      "Accuracy in  10  fold :  0.8139059304703476\n",
      "accuracy: 0.8287\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}